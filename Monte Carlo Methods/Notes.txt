here we do not assume complete knowledge of the environment but only experience(real or simulated)

model?
the model need only generate sample transitions, not the complete probability distributions of all possible
transitions 

Only on the completion of an episode are value estimates and policies changed.
MC methods can thus be incremental in an episode-by-episode sense, but not in
a step-by-step (online) sense. 

average rewards for each action. The main di↵erence is that now there are multiple states, each acting like a di↵erent
bandit problem (like an associative-search or contextual bandit)

we computed value functions from knowledge
of the MDP, here we learn value functions from sample returns with the MDP

update estimates on the basis of other estimates. We call this general idea bootstrapping MC doesnt bootstrap

first-visit MC method estimates v⇡(s) as the average of the returns following
first visits to s

On-policy methods attempt to evaluate or improve the
policy that is used to make decisions, whereas o↵-policy methods evaluate or improve
a policy di↵erent from that used to generate the data